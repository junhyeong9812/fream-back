# Fream Project Phase 1 Completion Review (2025.06.06)

## 프로젝트 개요
Spring Boot 기반의 확장 가능한 웹 서비스 백엔드를 구축하며, 자동화된 배포 파이프라인과 운영 효율성을 크게 개선한 프로젝트를 완료했습니다. 다양한 최신 기술 스택을 도입하여 마이크로서비스 아키텍처를 구현했지만, 완료 후 돌이켜보니 여러 아쉬운 점들이 있었습니다.

## 기술적 성과

### 구현한 주요 기능
- **CI/CD 파이프라인**: GitHub Actions를 활용한 자동화된 배포
- **마이크로서비스 아키텍처**: Docker 기반 컨테이너화
- **이벤트 기반 처리**: Kafka & Zookeeper를 통한 비동기 처리
- **검색 시스템**: Elasticsearch + Nori 형태소 분석기
- **캐싱 시스템**: Redis를 활용한 세션 및 데이터 캐싱
- **모니터링**: Kibana + Metricbeat 연동
- **인증 시스템**: OAuth2 소셜 로그인 (Google, Naver, Kakao)
- **결제 연동**: PortOne 결제 서비스
- **인프라**: AWS EC2 + Nginx + SSL

## 아쉬웠던 점들

### 1. 기술 스택 선택의 깊이 부족

**Kafka & Zookeeper 사용에 대한 회고**
- 로그 배치 처리와 이벤트 통제를 위해 Kafka 클러스터를 구성했지만, 과연 이것이 Kafka만의 고유한 해결책이었는지 의문이 듭니다.
- 메시지 큐나 인메모리 데이터 적재, HashMap을 활용한 키-값 기반 처리 방식으로도 동일한 결과를 얻을 수 있었을 것 같습니다.
- 문제 해결에 집착한 나머지 너무 많은 기술 스택을 얕게 사용한 것이 아닌가 하는 반성이 듭니다.

### 2. Spring Security OAuth2 구현의 한계

**모바일/웹 환경 대응**
- 모바일은 토큰 기반, 웹은 리다이렉트 기반으로 처리했지만, Spring Security 내부 동작 원리를 깊이 이해하지 못했습니다.
- LLM이 제공한 설정 방식을 그대로 따라했기 때문에 커스터마이징이 어려웠습니다.
- OAuth2.0 인증 프로세스에 대한 근본적인 이해가 부족했습니다.

### 3. 외부 서비스 연동의 표면적 구현

**결제 서비스 연동**
- PortOne 결제 서비스를 연동했지만, API 요청 플로우와 결제 검증 방식에 대한 깊은 이해가 부족했습니다.
- 단순 기능 구현에 그쳤고, 결제 보안이나 예외 상황 처리에 대한 고민이 부족했습니다.

**GPT-3.5 연동**
- API 연동은 성공했지만, 실제 비즈니스 로직과의 효과적인 결합에 대한 설계가 미흡했습니다.

### 4. Elasticsearch 활용의 아쉬움

**한계점**
- 단순 한글 분석기 용도로만 사용했지만, 실제로는 훨씬 강력한 기능들이 있었습니다.
- 1.2억 건 이상의 데이터 처리를 자바로 구현하다가 결국 파이썬 라이브러리로 전환한 경험이 있었는데, 너무 빠르게 포기한 것 같습니다.
- 자바 기반으로도 충분히 구현 가능했을 한글 분석기 시스템을 외부 라이브러리에 의존한 점이 아쉽습니다.

### 5. 인프라 관리의 표면적 접근

**Redis 활용**
- 세션과 캐싱 용도로만 사용했지만, MSA 구조에서의 인증 DB 역할까지 확장하지 못했습니다.
- Redis의 다양한 데이터 구조와 기능들을 제대로 활용하지 못했습니다.

**Nginx 설정**
- 로드밸런싱과 성능 튜닝을 하지 않고 단순 라우팅과 정적 파일 서빙만 구현했습니다.
- Nginx의 강력한 기능들을 제대로 활용하지 못한 점이 아쉽습니다.

### 6. 미완성된 기능들

**날씨 기반 옷 추천 시스템**
- Open-Meteo API를 통한 날씨 정보 수집은 구현했지만, 실제 추천 로직까지 완성하지 못했습니다.
- 스케줄링과 데이터 처리는 되었지만, 비즈니스 로직 완성도가 떨어졌습니다.

## 근본적인 문제 인식

### Spring Framework에 대한 이해 부족
가장 큰 문제는 **Spring이라는 거대한 시스템을 제대로 활용하지 못했다**는 점입니다. 다양한 기술 스택을 도입하는 데 집중한 나머지, 핵심이 되는 Spring의 철학과 설계 원칙을 깊이 이해하지 못했습니다.

### 기술 선택의 안일함
Java 코드로도 충분히 구현 가능한 기능들을 직접 시도해보지 않고, LLM을 통해 각 플랫폼 간 성능 비교만 예상해본 후 "비효율적이다"라는 추측에 의존했습니다. 최신 기술이 더 좋다는 막연한 위안감에 기대어 Java로 직접 구현하는 도전을 회피하고, 손쉬운 최신 플랫폼과 라이브러리를 선택하는 안일함이 컸습니다.

### 깊이 있는 학습의 필요성
Winter 프레임워크 프로젝트를 진행하면서 느낀 것은, 단순한 자바 코드로도 많은 것을 할 수 있다는 점입니다. 복잡한 기술 스택보다는 기본기를 탄탄히 하는 것이 더 중요하다는 것을 깨달았습니다.

## 향후 계획

### 단기 목표
1. **Spring Framework 심화 학습**: 내부 동작 원리와 설계 철학 이해
2. **기본기 강화**: 자바 기반의 문제 해결 능력 향상
3. **아키텍처 설계 능력**: 기술 선택의 근거와 트레이드오프 분석 능력 배양

### 장기 목표
Winter 프레임워크 프로젝트 완료 후, 이번 Fream 프로젝트를 다시 업그레이드할 예정입니다. 이때는 각 기술의 필요성을 명확히 정의하고, 깊이 있는 구현을 통해 진정한 의미의 완성도 높은 프로젝트를 만들어보겠습니다.

## 결론

이번 프로젝트를 통해 다양한 기술 스택에 대한 접근성과 이해도는 늘어났지만, 정작 중요한 것은 **왜, 어떻게**에 대한 깊은 고민이었다는 것을 깨달았습니다. 기술은 도구일 뿐이고, 문제를 정확히 파악하고 적절한 해결책을 선택하는 능력이 더 중요하다는 교훈을 얻었습니다.

앞으로는 "많은 기술을 아는 개발자"가 아닌 "문제를 제대로 해결할 수 있는 개발자"가 되기 위해 기본기부터 차근차근 다져나가겠습니다.

# Fream 프로젝트 솔직한 후기
```
해당 프로젝트를 진행함에 있어서 사용한 기술 스택은 많았지만, 결국 내가 이 기술 스택을 제대로 다루고 있는지 의문이 들었다. 

나름 Kafka와 Zookeeper를 통한 컨슈머와 프로듀서를 활용한 로그 배치 처리나 이벤트 통제, 그리고 각 클러스터를 통해 각 컨슈머 프로듀서마다의 역할을 분할하여 이를 통해 배치 구조 처리를 했지만, 과연 이것이 Kafka를 통해서만 가능했던 건지 의문이다. 

메시지 큐를 통해 log 데이터나 배치 처리할 데이터에 대한 데이터를 메모리에 쌓아서 처리할 수도 있었을 것이고, HashMap 형식으로 요청들을 저장해놓고 해당 키를 통해 특정 키가 들어올 때마다 데이터를 적재하고 HashMap이나 객체를 생성해서 이를 처리할 수 있도록 할 수 있지 않았을까? 라는 고민을 안 해본 점이 오히려 현재 개발 공부를 지속적으로 하면서 느끼게 되었다. 

그때는 현재 문제를 최대한 효율적으로 다룰 수 있는 도구에 집착한 나머지 너무 많은 기술 스택들을 얕게 사용해본 게 아닌가 싶은 느낌이었다. 

Winter 프로젝트를 통해 단순한 자바 코드로도 많은 것을 할 수 있다는 걸 알게 되었고, 사실 내가 사용했던 것들을 보면 OAuth2.0 인증을 통한 사용자 주요 정보를 외부 대기업에 위임하여 위험도를 줄이고 사용자에게 편리함을 제공할 수 있어서 구현했지만, 이 과정에서 모바일 환경과 웹 환경에서 모바일은 해당 값을 리다이렉션을 통해 넘기는 게 아닌 토큰값으로 넘기고 웹에서는 리다이렉트 주소를 통해 로그인 처리를 인가받지만, 이 부분을 Security 코드 내부를 분석하지 않고 그냥 LLM이 알려준 방식대로 하나씩 Spring의 고유 설정들로 구현만 했기에 단순 구현이나 목적만 있었으며 해당 내용의 프로세스를 제대로 알지 못한 방식이었기에 커스터마이징이 힘들었다.

GPT-3.5 연동이나 PortOne 결제 서비스 연동 같은 경우 API 요청 플로우에 대한 공부와 결제 서비스에 대한 검증 방식에 대해 프로세스를 바로잡지 못한 부분이 상당히 아쉽다고 생각했다.

또한 Open-Meteo의 무료 날씨 정보를 스케줄을 통해 데이터를 받아서 해당 날씨 정보를 통해 날씨마다 사용자에게 옷을 추천해주는 시스템을 구축하려고 했지만 해당 부분을 하지 못한 게 아쉽다.

또한 Elasticsearch의 경우 단순 한글 분석기로 사용하기 위해서 사용했지만, 실무에서 ES에 데이터 마이그레이션 작업과 1.2억 건 이상의 데이터의 가공 및 연산 처리를 하는 과정에서 단순 분석기는 정말 작은 부분이고 실제 Elasticsearch에 쓰인 다양한 알고리즘 기법이나 이러한 시스템을 자바로 구축했지만, 이걸 며칠 동안 자바로 구축하다가 결국 파이썬 라이브러리로 구현하여 연동하려고 한 점에 있어서도 상당히 아쉬웠다. 자바 기반 Elasticsearch가 된다면 분명 자바 기반으로 내가 기존에 하려고 했던 한글 분석기 시스템을 자바로 만들 수 있었을 텐데 너무 빠르게 포기하고 파이썬 라이브러리나 ES에 의존한 게 아닌가라는 생각이 들었다.

현재는 Kibana 모니터링 시스템과 Metricbeat 시스템을 연계해서 쓰지만 해당 부분이나 서버 안정성에 대한 생각이 너무 짧았으며, Redis를 세션과 사용자 정보 캐싱으로 사용했지만 해당 부분도 Redis를 추후 MSA 구조로 바꿀 때 인증 정보 처리를 위해서 인증 DB 형식으로 사용하려고 했지만 해당 부분도 구현하지 못한 게 상당히 아쉬웠다.

또한 Nginx를 통해 현재는 내 윈도우 PC에서 여러 서버를 구동 중이지만 로드밸런싱과 Nginx 튜닝을 하지 않고 단순히 라우팅 역할과 단순 URL에 대한 캐싱 처리와 정적 파일 서빙만 하고 있는 게 상당히 아쉽다. 

각 기술마다 가진 너무나도 많은 장점을 제대로 활용하지 못하고 단순히 구현만 했다는 점이 아쉽고, 이런 걸 다양하게 쓰다 보니 다양한 기술들에 대한 접근성과 이해도는 늘어났지만 정작 중요한 Spring이라는 거대한 시스템을 제대로 활용 못한 부분이 와닿는다. 

해당 프로젝트는 현재 진행 중인 Winter 프레임워크 프로젝트 이후 다시 업그레이드해야 될 것 같다.
```
